{"cells":[{"cell_type":"markdown","source":["# Table of contents\n","1. [Introduction](#introduction)\n","2. [QuickStart](#quickstart)\n","3. [Modelling](#modelling)\n","    1. [Variational Autoencoder](#vae)\n","    2. [Model architecture](#architecture) \n","    3. [Encoder](#encoder)\n","    4. [Decoder](#decoder)\n","    5. [VariationalAutoEncoder](#vaecode)\n","    6. [Autoencoder](#autoencoder)\n","    7. [Factory class](#factory) \n","4. [Evaluation](#eval)\n","5. [Discussion](#discuss)\n"],"metadata":{}},{"cell_type":"markdown","source":["# Introduction <a name=\"introduction\"></a>\n","\n","In this notebook we consider the task of developing a generative model for creating synthetic data. We briefly examine the main object ```Factory``` in the ```aidatafactory``` library which is to be able to generate artificial data while preserving the statistical properties of the real data."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# install libraries\n","!pip install pandas -q gwpy\n","!pip install aidataFactory -q gwpy\n","!pip install matplotlib==3.1.3 -q gwpy"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# load libraries\n","import pandas as pd\n","from aidatafactory import Factory"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["# Quickstart <a name=\"quickstart\"></a>\n","In this section we run quickly through the functionality of the <b>Factory</b> class and generate synthetic data. This can take up to 8 minutes. Further below you can read more details about implementation.\n","\n","#### Step 1. Run the cell below to load data.\n"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import os\n","url = 'https://github.com/ngocuong0105/dataFactory/blob/master/showcase/income_data.csv'\n","income_data = pd.read_csv(url, error_bad_lines=False); income_data"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["#### Step 2. Instantiate Factory object. Need to specify numerical and categorical columns."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["factory = Factory(columns_spec = {'numerical':['Income'], 'categorical':['Gender']})"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Step 3. Train generative model.\n","Pregress of fitted models for each category are plotted."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["factory.learn(income_data)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Step 4. Generate as many as you wish artificial samples."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["ai_data = factory.generate(num_rows = len(income_data), data = income_data);ai_data"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Step 5. Compare distributions."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","for gender in income_data['Gender'].unique():\n","    fig = plt.figure(figsize=(7,4))\n","    real = income_data[income_data['Gender']==gender]\n","    synth = ai_data[ai_data['Gender']==gender]\n","    sns.histplot(real['Income'], x = real['Income'], element=\"poly\",  stat = 'density',alpha = 1)\n","    sns.histplot(synth['Income'], x = synth['Income'], element=\"poly\",  stat = 'density', color= 'purple', alpha = 0.5)\n","    fig.legend(labels=['Real','Artificial'])\n","    plt.title('Real vs Artificial')\n","    plt.xlim(synth['Income'].quantile(0.05), synth['Income'].quantile(0.95))\n","    plt.show()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["# Modelling <a name=\"modelling\"></a>\n","In the next sections we describe the mathematics behind the output <b>a distribution</b> which will model the real data as closely as possible and we show how we use this distribution to sample synthetic data points. We make a key assumption about the dataset:\n","- assume that all rows in the table are independent\n","\n","Note for time-series datases or those where there is conditional dependence between rows this assumtion would be violated. Typically in literature (and practice) the assumption reads \"independent and identically distributed (i.i.d.)\", however, here we ommited the identical distribution part as we saw that for different effort categories we have a different distribution (normal, bernoulli, mixed).\n","\n","In the next few subsections we describe a well-known machine-learning model Variational Autoencoder and discuss why it is suitable for our task of learning a distribution for creating synthetic data from our real dataset.\n","\n","### Autoencoder <a name=\"autoencoder\"></a>\n","An autoencoder is a neural network that is trained to attempt to copy its input\n","to its output (almost what we want). It has a hidden layer that describes a code used to\n","represent the input in a lower dimensional latent representation. Then the autoencoder decodes the latent representation back to a reconstructed term. The network may be viewed as consisting of two parts: an encoder function $h = f(x)$ and a decoder that produces a reconstruction term $r = g(h)$. If an autoencoder succeeds in simply\n","learning to set $g(f (x)) = x$ everywhere (i.e. the identity function), then it is not especially useful. Instead, autoencoders are designed to be unable to learn to copy perfectly (this is what we want).\n","\n","\n","### Variational Autoencoder <a name=\"vae\"></a>\n","For our task recall that we need as an output a generative model from which we would sample synthetic data. Using just an autoencoder is not enough since we would learn the functions $f$ and $g$ which are deterministic. We need stachastic mappings versions of $f$ and $g$ and this naturally extends to the use of Variatioonal Autoencoders also known as VAE - our choice of architecture.\n","\n","A VAE is a probabilistic take on the autoencoder, a model which takes high dimensional input data and compresses it into a smaller representation. Unlike a traditional autoencoder, which maps the input onto a latent vector, a VAE maps the input data into the parameters of a probability distribution, such as the mean and variance of a Gaussian. Using it we will learn to compress the data while minimizing the reconstruction error. The two main reasons why we choose to work with VAE for this task are:\n","\n","- Simplicity. The encoding and decoding operations in VAE naturally allow us to create synthetic data.\n","- Bayesian inference. We are allowed to put priors on the encoder and decoder which makes the model customizable to each dataset. We will log-transform our data and set normal priors as we saw the log data follows normal.\n","\n","### Model architecture: <a name=\"architecture\"></a>\n","VAE has 2 main parts - encoder and decoder. We define for each data item $i = 1,2,3,...,n:$\n","- Let $Z_i$ be a (latent) $k$-dimensional normally distributed random vector with mean\n","0 and identity covariance: $Z_i ∼ N(0,I_k)$\n","- Given $Z_i$ , the distribution of the $i$-th data item is a $p$-dimensional nonlinear and non-Gaussian: $X_i |Z_i ∼ p_\\theta(·|Z_i )$, where  $p_\\theta(·|Z_i )$ is the <b>decoder</b>.\n","For example, $p_\\theta(·|Z_i)$ can be a Gaussian with mean $\\mu_\\theta(Z_i)$ and diagonal variance $\\sigma^2_\\theta (Z_i)$,\n","where both $\\mu_\\theta$ and $\\sigma^2_\\theta$ are outputs of a neural network with parameters $\\theta$. For another\n","example, $p_\\theta(X_i|Z_i)$ can be an independent Bernoulli for each element of $X_i$ , with mean $\\mu_\\theta(Z_i)\\in(0, 1)$, with the range restricted to $(0, 1)$ using a <b>sigmoid</b> output nonlinearity. All these possibilities for the decoder are added as a functionality in the <b>Decoder</b> class. Note that if we choose simple linear function for $p_\\theta(·|Z_i )= \\mu+\\sigma Z_i$, we will have probabilistic PCA which is a basic linear dimensionality reduction technique. \n","- Given the neural network in the generative model, the exact posterior\n","distribution $p(z|x)$ will typically not be tractable anymore. Instead we consider a variational\n","distribution $q_\\phi(z|x)$, called a recognition model or <b>encoder</b>, which is itself given by a\n","neural network with parameters φ. The encoder is typically a Gaussian with a diagonal\n","covariance matrix, $q_\\phi(z|x) = N (z; \\mu_\\phi(x), \\sigma_\\phi(x))$\n","\n","Next we define a metric which we would optimise while training our model.\n","\n","<b>Definition</b>. Variational free energy in a latent variable model $p(x, z|\\theta)$ is defined as $F(\\theta, q) = E_q (log p(x, z|\\theta) − log q(z))$. It is a natural lower bound of the log-likelihood (evidence): \n","\n","log$p(x|θ) ≥ E_{z∼q}$ log$\\frac{p(x,z|\\theta)}{q(z)} = E_{z∼q}$ log$ p(x, z|\\theta) − E_{z∼q} $log $q(z)$\n","\n","The variational free energy is also known as ELBO (evidence lower bound).\n","\n","In our notebook we will minimize the difference between the likelihood and the ELBO. Other implementations might maximize ELBO. Finally, we will consider separate model for each type of 'effort' because we saw that they have different distributions, as a result we develop a separate Variational Autoencoder for each 'effort' category .\n","\n","### Coding architecture\n","We will build our own VariationalAutoencoder using the tenserflow.keras model subclassing API. We choose this architecture as it is the most flexible option allowing us to build fully-customizable models. Also this architecture follows closely the OOP paradigm which (imo) enhances code readability"],"metadata":{}},{"cell_type":"markdown","source":["### Encoder <a name=\"encoder\"></a>\n","The encoder class encapsulates the functionality of the variational distribution or recognition model $q_\\phi(z|x) = N (z; \\mu_\\phi(x), \\sigma_\\phi(x))$. We define 2 shallow (one for the mean and one for the variance) feed-forward neural networks each with one layer. In reality, we do not create neural network for the variance but rather for the log-variance. This is because variance has to be strictly positive whereas log-variance can be both negative and positive, making it easier to model with a neural network without the need for additional constraints.\n","\n","The ELBO objective can be rewritten (see VariationalAutoEncoder section for derivation):\n","\n","ELBO = $E_{q_\\phi(z|x)}($ log $p_\\theta(x|z)) − KL (q_\\phi(z|x)||p(z))$,\n","\n","where the second therm denotes KL-divergence (measure of distance between distributions).\n","\n","During training we need to estimate the first term $E_{q_\\phi(z|x)}($ log $p_\\theta(x|z))$ which requires sampling from $q_\\phi(z|x)$. Getting a good Monte Carlo estimator would require drawing many samples of codes for each obervation $x_i$ (i.e sampling from decoder) , which is prohibitive and slow. A simple solution to this is the reparametrization trick. For example, in the case of a normal variational posterior, a draw $z_i ∼ N (z; \\mu_\\phi(x), \\sigma_\\phi(x))$\n","can be written as $z_i = \\mu_\\phi(x) + \\sigma_\\phi(x) \\epsilon$, with $\\epsilon ∼ N (0, I)$. The reparametrization function is available in the <b>Sampling</b> class."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import tensorflow as tf\n","import tensorflow_probability as tfp\n","from tensorflow.keras import Model\n","from tensorflow.keras import layers, initializers"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["class Encoder(layers.Layer):\n","    \"\"\" \n","    Maps input data tensor to latent triple vector (z_mean, z_log_var, z)\n","    \"\"\"\n","\n","    def __init__(self, latent_dim: int, name:str = \"encoder\", **kwargs):\n","        super(Encoder, self).__init__(name=name, **kwargs)\n","        self.dense_mean = layers.Dense(latent_dim, bias_initializer='ones') # linear activations\n","        self.dense_log_var = layers.Dense(latent_dim, kernel_initializer = initializers.RandomNormal(stddev=0.1)) # use logvar as var need constaint to be positive\n","        self.sampling = Sampling()\n","\n","    def call(self, inputs: tf.Tensor):\n","        x = inputs\n","        z_mean = self.dense_mean(x) # \n","        z_log_var = self.dense_log_var(x)\n","        z = self.sampling.reparametrization((z_mean, z_log_var))\n","        return z_mean, z_log_var, z\n"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["### Decoder <a name=\"decoder\"></a>\n","The decoder class encapsulates the functionality of the output of the generative model or decoder distribution $p_\\theta(·|Z_i )$. Its goal is to capture any non-linear relationship and reconstruct the compressed data.\n","Our Decoder class supports 4 different decoding distributions:\n","1. Simple relu decoder - one layer neural network with relu activation, we zeroe out negative values as MonthlyIncome is non-negative.\n","2. Normal decoder - one layer for the mean and one layer for the log-variance.\n","3. Bernoulli decoder - one layer for the mean (added for 0-1 outputs).\n","4. Gamma decoder <b>(under development - manual hypertuning recommended )</b> - one layer for the shape parameter and one for the rate parameter. \n","\n"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["class Decoder(layers.Layer):\n","    \"\"\"\n","    Converts z, the encoded tensor, back into a reconstructed term.\n","    \"\"\"\n","\n","    def __init__(self, original_dim: int, distribution: str = 'default', name: str = 'decoder', **kwargs):\n","        super(Decoder, self).__init__(name=name, **kwargs)\n","        self.distribution = distribution\n","        self.sampling = Sampling()\n","\n","        # simple relu decoder (default)\n","        self.dense_output = layers.Dense(original_dim, activation = 'relu') # dont want negative values\n","\n","        # normal  decoder\n","        self.dense_mean = layers.Dense(original_dim, bias_initializer='ones') # linear activations\n","        self.dense_log_var = layers.Dense(original_dim, kernel_initializer = initializers.RandomNormal(stddev=0.1))\n","        \n","        # bernoulli decoder\n","        self.bernoulli_lambda = layers.Dense(original_dim, activation = 'sigmoid')\n","\n","        # gamma decoder\n","        self.gamma_log_alpha = layers.Dense(original_dim, bias_initializer='ones', kernel_initializer = initializers.RandomNormal(stddev=0.1)) # use logvar as var need constaint to be positive\n","        self.gamma_log_beta = layers.Dense(original_dim, bias_initializer='zeros', kernel_initializer = initializers.RandomNormal(stddev=0.1)) # use logvar as var need constaint to be positive\n","        \n","    def call(self, inputs: tf.Tensor):\n","        if self.distribution == 'default':\n","            return self.dense_output(inputs)\n","\n","        elif self.distribution == 'normal':\n","            z = inputs\n","            mean = self.dense_mean(z)\n","            log_var = self.dense_log_var(z)\n","            output = self.sampling.normal((mean,log_var))\n","            return output\n","\n","        elif self.distribution == 'bernoulli':\n","            z = inputs\n","            mean = self.bernoulli_lambda(z)\n","            output = self.sampling.bernoulli(mean)\n","            output = tf.cast(output, tf.float32) \n","            return output\n","            \n","        elif self.distribution == 'gamma':\n","            z = inputs\n","            log_alpha = self.gamma_log_alpha(z)\n","            log_beta = self.gamma_log_beta(z)\n","            output = self.sampling.gamma((log_alpha,log_beta))\n","            return output\n"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["### VariationalAutoEncoder <a name=\"vaecode\"></a>\n","VariationalAutoEncoder class provides end-to-end interface for encoding and decoding into one place. In the call method we add the KL-divergence as the ELBO bound can be rewritten as:\n","\n","ELBO =  $E_q ($log $p(x, z|\\theta) − $log$ q(z)) $<br>$\n","     = E_q ($log$ p(x|z,\\theta) + E_q ($log$ p(z)) − E_q ($log$ q(z|x)) $ <br>\n","         $ = E_{q_\\phi(z|x)}($ log $p_\\theta(x|z)) − KL (q_\\phi(z|x)||p(z))$\n","         \n","The first term is the reconstruction error and the second term is the KL-distance between the true encoding distribution $p(z)$ and the variational distribution  $q_\\phi(z|x)$ which estimates the encoder."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["class VariationalAutoEncoder(Model):\n","    \"\"\"\n","    Combines the encoder and decoder into one model for training.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\\\n","        original_dim: int,\\\n","        latent_dim: int,\\\n","        distribution: str,\\\n","        name:str = \"variational autoencoder\",\\\n","        **kwargs\\\n","    ):\n","        super(VariationalAutoEncoder, self).__init__(name = name, **kwargs)\n","        self.original_dim = original_dim\n","        self.encoder = Encoder(latent_dim = latent_dim)\n","        self.decoder = Decoder(original_dim, distribution = distribution)\n","\n","    def call(self, inputs):\n","        z_mean, z_log_var, z = self.encoder(inputs) \n","        reconstructed = self.decoder(z)\n","        # Add KL divergence regularization loss.\n","        kl_loss = -0.5 * tf.reduce_mean(\n","            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n","        )\n","        self.add_loss(kl_loss)\n","        return reconstructed"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["### Factory <a name=\"factory\"></a>\n","\n","The Factory class is a little bit longish, so we will not attach it here. Please refer to it in the [git repository](https://github.com/ngocuong0105/dataFactory) \n","Recall that in the QuickStart we instantiated Factory object like that:\n"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["factory = Factory(columns_spec = {'numerical':['Income'], 'categorical':['Gender']})"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["Infact the class <b>Factory</b> supports more arguments which could come in handy when experimenting different models and fine-tuning parameters:"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["factory = Factory(\n","                columns_spec = {'numerical':['Income'], 'categorical':['Gender']},\n","                distribution_spec = {'Male':'gamma','Female':'noemal'},\n","                log_transform = True,\n","                latent_dim = 32,\n","                batch_size = 64,\n","                epochs = 15,\n","                learning_rate = 1e-3\n","            )\n","factory.learn(income_data)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["Key parameter which we have <b> not </b> used in the QuickStart is <b>distribution_spec</b>. Our generative model have assumed independent distributions between rows in the table but allows non-identical distributions. <b>distribution_spec</b> is a dictionary where the keys are the category values (Male or Female category) and the dictionary values are the decoder distributions ('default','bernoulli','normal','gamma' - see Decoder section for more information). Above we assign <b>distribution_spec</b> in Factory to have gamma distribution Income for Male and normal distribution for Female."],"metadata":{}},{"cell_type":"markdown","source":["Next generate synthetic samples:"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["ai_data = factory.generate(len(income_data), income_data)"],"outputs":[],"metadata":{"scrolled":true,"trusted":true}},{"cell_type":"markdown","source":["# Evaluation <a name=\"eval\"></a>\n","\n","The evaluation of the generated artificial data can be assessed by evaluating the effectiveness of machine learning tasks. Models that are trained on the artificial data can be compared with models trained on the original data, and scored on criteria such as accuracy metrics. In our particular problem we have not defined concrete machine learning task - thus we will simply compare the plots and descriptive of the original and artificial datasets."],"metadata":{}},{"cell_type":"markdown","source":["Consider the plot comparision between artificial and real data overall:"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["income_data.groupby('Gender').describe()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["ai_data.groupby('Gender').describe()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","for gender in income_data['Gender'].unique():\n","    fig = plt.figure(figsize=(7,4))\n","    real = income_data[income_data['Gender']==gender]\n","    synth = ai_data[ai_data['Gender']==gender]\n","    sns.histplot(real['Income'], x = real['Income'], element=\"poly\",  stat = 'density',alpha = 1)\n","    sns.histplot(synth['Income'], x = synth['Income'], element=\"poly\",  stat = 'density', color= 'purple', alpha = 0.5)\n","    fig.legend(labels=['Real','Synthetic'])\n","    plt.title('Real vs Synthetic')\n","    plt.xlim(synth['Income'].quantile(0.05), synth['Income'].quantile(0.95))\n","    plt.show()"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Successful evaluation metrics need to choose should punish if the original data is really different from the synthetic data and also punish if they are too similar as identical datasets are useless.  Our goal is to measure synthetic data effectiveness when in use compared to the original data. There are several groups of evaluation metrics:\n","\n","- Statistical metrics: These metrics compare the statistical properties of the synthetic and original datasets. Includes histogram plots, descriptive statistics analysis, hypothesis testing for distributional difference.\n","\n","- Predictive power metrics: These metrics compare the performance of Machine Learning alforithm trained on synthetic and on original datasets. \n","\n","- Likelihood metrics: These metrics fit probabilistic model to real data and estimate the chance that the synthetic data comes from this distribution.\n","\n","- Detection metrics: These metrics try to train a Machine Learning Classifier that learns to distinguish the real data from the synthetic data, and report a score of how successful this classifier is.\n"],"metadata":{}},{"cell_type":"markdown","source":["# Discussion <a name=\"discuss\"></a>"],"metadata":{}},{"cell_type":"markdown","source":["We have created our own generative model Variational Autoencoder with which we sampled artificial data. From the\n","comparison plots of the artificial and original data distributions above we see that the artificial data is somewhat more monotone and systemically has lower mean. More experiments on the initialization of parameters of the encoder and decoder are needed as so far we have only eye balled what the correct initialization should be. Also the real data set is only with integers, so rounding our results could be helpful.  As for evaluation, without having a specific machine learning task (e.g. regression, classification) we cannot use predictive metrics to quantify how good is our artificial dataset. However, if we have a certain task then a good idea for evaluation would be to benchmark our artificial data generation against the original dataset added with some Gaussian noise. \n","\n","Finally, it is worth to note that although VAE is simple and easy to understand, it cannot capture the case of dependent rows in data (e.g time-series). Also Variational Autoencoders and its extensions suffer from scalability issues. It mainly results from the costly decoding operations involved in the graph reconstruction where we multiply dense matrices. Therefore generating artificial data when working with large databases and multiple tables can be an issue. For timely structured data synthesis literature suggests usage of LSTM and for better scalability utilization of other directed and undirected graphical models such as GANs.\n"],"metadata":{}}],"metadata":{"interpreter":{"hash":"d680aad21ea2b054fb992d69bb2b335811c98531f60053f70f827dbc313188ad"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"}},"nbformat":4,"nbformat_minor":2}